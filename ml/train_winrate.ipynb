{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e74199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad856cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = [10, 20]\n",
    "LOGGING_INTERVAL = 500\n",
    "MAX_GRAD_NORM = 0.1\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc68deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZES = [512]\n",
    "BATCH_SIZE = BATCH_SIZES[0]\n",
    "MAX_PHYSICAL_BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56bdd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "def init_roberta(model_name=\"roberta-base\", num_labels=2):\n",
    "    \"\"\"\n",
    "    Initialize a RoBERTa model + tokenizer for sequence classification.\n",
    "    \"\"\"\n",
    "    # Load config with custom number of labels\n",
    "    config = RobertaConfig.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "    )\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        do_lower_case=False,\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = init_roberta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28110db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trainable_layers(model, tokenizer):\n",
    "\n",
    "  trainable_layers = [\n",
    "      model.roberta.encoder.layer[-1],\n",
    "      model.classifier\n",
    "  ]\n",
    "\n",
    "  total_params = 0\n",
    "  trainable_params = 0\n",
    "\n",
    "  for p in model.parameters():\n",
    "      p.requires_grad = False\n",
    "      total_params += p.numel()\n",
    "\n",
    "  for layer in trainable_layers:\n",
    "      for p in layer.parameters():\n",
    "          p.requires_grad = True\n",
    "          trainable_params += p.numel()\n",
    "\n",
    "make_trainable_layers(model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89766244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers.data.processors.utils import InputExample\n",
    "from transformers.data.processors.glue import glue_convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSets(tokenizer):\n",
    "  LABEL_LIST = [0,1]\n",
    "\n",
    "  # ---- Compute a safe max length ----\n",
    "  all_texts = pd.concat([train_file['text'], test_file['text'], poison_test_file['text']])\n",
    "  # Use tokenizer to count ids (no special tokens here; weâ€™ll add a tiny buffer)\n",
    "  token_lengths = []\n",
    "  for t in all_texts:\n",
    "      t = \"\" if not isinstance(t, str) else t\n",
    "      token_lengths.append(len(tokenizer.encode(t, add_special_tokens=True)))\n",
    "  raw_max = max(token_lengths) + 2\n",
    "\n",
    "  # Some tokenizers return a huge sentinel for model_max_length; clamp with 512 fallback\n",
    "  model_max = getattr(tokenizer, \"model_max_length\", 512)\n",
    "  if model_max is None or model_max > 100000:  # handle sentinel\n",
    "      model_max = 512\n",
    "  MAX_SEQ_LENGTH = min(raw_max, model_max)\n",
    "\n",
    "  def _create_examples(df, set_type):\n",
    "    examples = []\n",
    "    for index, row in df.iterrows():\n",
    "      if row[\"label\"] not in LABEL_LIST:\n",
    "        continue\n",
    "      if not isinstance(row[\"text\"], str):\n",
    "        continue\n",
    "\n",
    "      guid = f\"{index}-{set_type}\"\n",
    "      examples.append(\n",
    "          InputExample(guid=guid, text_a=row[\"text\"], label=row[\"label\"])\n",
    "      )\n",
    "    return examples\n",
    "\n",
    "  def _df_to_features(df, set_type):\n",
    "    examples = _create_examples(df, set_type)\n",
    "    legacy_kwards = {}\n",
    "    from packaging import version\n",
    "    if version.parse(transformers.__version__) < version.parse(\"2.9.0\"):\n",
    "      legacy_kwards = {\n",
    "          \"pad_on_left\": False,\n",
    "          \"pad_token\": tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "          \"pad_token_segment_id\": 0,\n",
    "      }\n",
    "    return glue_convert_examples_to_features(\n",
    "        examples = examples,\n",
    "        tokenizer = tokenizer,\n",
    "        label_list = LABEL_LIST,\n",
    "        max_length = MAX_SEQ_LENGTH,\n",
    "        output_mode = \"classification\",\n",
    "        **legacy_kwards,\n",
    "    )\n",
    "\n",
    "  def _features_to_dataset(features):\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype = torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype = torch.long)\n",
    "\n",
    "    all_labels = torch.tensor([f.label for f in features], dtype = torch.long)\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_labels) # Removed all_token_type_ids\n",
    "    return dataset\n",
    "\n",
    "  train_features = _df_to_features(train_file, \"train\")\n",
    "  test_features = _df_to_features(test_file, \"test\")\n",
    "  poison_test_features = _df_to_features(poison_test_file, \"test\")\n",
    "\n",
    "  train_dataset = _features_to_dataset(train_features)\n",
    "  test_dataset = _features_to_dataset(test_features)\n",
    "  poison_test_dataset = _features_to_dataset(poison_test_features)\n",
    "\n",
    "  return train_dataset, test_dataset, poison_test_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
